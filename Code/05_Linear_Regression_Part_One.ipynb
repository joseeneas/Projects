{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/MIT_logo.jpg width=150 align=right />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROFESSIONAL CERTIFICATE IN DATA SCIENCE AND ANALYTICS\n",
    "# Linear Regression: Part One\n",
    "This code snippet imports several essential Python libraries and modules commonly used for data analysis, statistical modeling, and visualization. Each library serves a specific purpose in the context of linear regression and related tasks.\n",
    "\n",
    "1. **`numpy` (`import numpy as np`)**:\n",
    "   - NumPy is a fundamental library for numerical computing in Python. It provides support for multi-dimensional arrays and a wide range of mathematical operations. The alias `np` is a standard convention for referencing NumPy functions. In the context of linear regression, NumPy is often used for handling numerical data, performing matrix operations, and generating random data.\n",
    "\n",
    "2. **`pandas` (`import pandas as pd`)**:\n",
    "   - Pandas is a library designed for data manipulation and analysis. It introduces data structures like `DataFrame` and `Series`, which make it easy to handle structured data. The alias `pd` is a standard convention for referencing Pandas functions. In this context, Pandas is likely used to organize and preprocess datasets for regression analysis.\n",
    "\n",
    "3. **`scipy.stats` (`from scipy import stats`)**:\n",
    "   - The `stats` module from SciPy provides a wide range of statistical functions and distributions. It is often used for hypothesis testing, calculating statistical measures, and working with probability distributions. In regression analysis, `stats` can be used to evaluate model assumptions or compute statistical metrics.\n",
    "\n",
    "4. **`matplotlib.pyplot` (`from matplotlib import pyplot as plt`)**:\n",
    "   - Matplotlib is a popular library for creating visualizations. The `pyplot` module provides a MATLAB-like interface for generating plots. The alias `plt` is a standard convention for referencing `pyplot`. In the context of linear regression, Matplotlib is typically used to create scatter plots, visualize regression lines, and display residuals.\n",
    "\n",
    "5. **`statsmodels.formula.api` (`import statsmodels.formula.api as smf`)**:\n",
    "   - Statsmodels is a library for statistical modeling and hypothesis testing. The `formula.api` module provides a high-level interface for specifying models using formulas (e.g., `y ~ x1 + x2`). This is particularly useful for performing linear regression and obtaining detailed model summaries, including coefficients, p-values, and RÂ² values.\n",
    "\n",
    "6. **`sklearn.linear_model.LinearRegression` (`from sklearn.linear_model import LinearRegression`)**:\n",
    "   - Scikit-learn is a machine learning library that provides tools for building and evaluating models. The `LinearRegression` class is used to perform linear regression using an efficient implementation. It supports fitting models, making predictions, and accessing model parameters like coefficients and intercepts.\n",
    "\n",
    "### Purpose:\n",
    "These imports collectively provide the tools needed to perform linear regression, preprocess data, visualize results, and evaluate model performance. By combining these libraries, the code can handle tasks ranging from data preparation to advanced statistical analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                                           as np  # type: ignore\n",
    "import pandas                                          as pd  # type: ignore\n",
    "from   scipy                   import stats                   # type: ignore\n",
    "from   matplotlib              import pyplot           as plt # type: ignore\n",
    "import statsmodels.formula.api                         as smf # type: ignore\n",
    "from   sklearn.linear_model    import LinearRegression        # type: ignore\n",
    "import pprint                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `abline`, is designed to plot a straight line on an existing Matplotlib plot using a given slope and intercept. It is particularly useful for visualizing linear relationships, such as regression lines, over a scatter plot or other data visualizations.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Retrieve the Current Axes**:\n",
    "   - `axes = plt.gca()`:\n",
    "     - This retrieves the current axes object from the active Matplotlib figure using the `gca()` (get current axes) function. The axes object represents the plotting area and provides access to its properties, such as the x-axis limits.\n",
    "\n",
    "2. **Get the X-Axis Limits**:\n",
    "   - `x_vals = np.array(axes.get_xlim())`:\n",
    "     - The `get_xlim()` method of the axes object returns the current x-axis limits as a tuple `(left, right)`. These limits define the range of x-values over which the line will be plotted.\n",
    "     - The `np.array()` function converts this tuple into a NumPy array, which allows for efficient mathematical operations.\n",
    "\n",
    "3. **Calculate the Corresponding Y-Values**:\n",
    "   - `y_vals = intercept + slope * x_vals`:\n",
    "     - Using the equation of a straight line, `y = intercept + slope * x`, the function calculates the corresponding y-values for the x-axis limits. This ensures that the line spans the entire width of the plot.\n",
    "\n",
    "4. **Reshape the X and Y Values**:\n",
    "   - `x_vals, y_vals = x_vals.reshape(-1, 1), y_vals.reshape(-1, 1)`:\n",
    "     - The `reshape(-1, 1)` method reshapes the x and y arrays into column vectors. While this step is not strictly necessary for plotting, it may be included to ensure compatibility with other operations or conventions in the code.\n",
    "\n",
    "5. **Plot the Line**:\n",
    "   - `plt.plot(x_vals, y_vals, '--', color=color)`:\n",
    "     - The `plot()` function is used to draw the line on the current plot. The arguments specify:\n",
    "       - `x_vals` and `y_vals`: The x and y coordinates of the line.\n",
    "       - `'--'`: A dashed line style.\n",
    "       - `color=color`: The color of the line, which is passed as an argument to the function.\n",
    "\n",
    "### Purpose:\n",
    "The `abline` function is a utility for adding a straight line to a plot, typically to represent a linear model or trend. By using the current x-axis limits, it ensures that the line fits seamlessly within the existing plot. The ability to customize the line's color makes it versatile for highlighting different relationships or comparisons in a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(intercept, slope, color):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    x_vals, y_vals = x_vals.reshape(-1, 1),y_vals.reshape(-1, 1)\n",
    "    plt.plot(x_vals, y_vals, '--', color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "$$y = b + wx$$\n",
    "\n",
    "We want to find an optimal set of values for $w$ and $b$ that minimize the residuals.\n",
    "This code snippet demonstrates the process of performing simple linear regression using synthetic data, visualizing the results with a scatter plot and a regression line.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Generate Synthetic Data**:\n",
    "   - `x = np.random.uniform(0, 10, 20)`:\n",
    "     - This generates 20 random values for `x` uniformly distributed between 0 and 10.\n",
    "   - `y = 10 + x * 2 + np.random.randn(20) * 2`:\n",
    "     - The corresponding `y` values are generated using the linear equation `y = 10 + 2x` with added random noise. The noise is drawn from a standard normal distribution (`np.random.randn`) and scaled by a factor of 2 to introduce variability.\n",
    "\n",
    "2. **Reshape Data**:\n",
    "   - `x, y = x.reshape(-1, 1), y.reshape(-1, 1)`:\n",
    "     - The `reshape` method converts the 1D arrays `x` and `y` into 2D column vectors with one column and multiple rows. This is required because the `LinearRegression` model expects 2D input for features (`x`) and target values (`y`).\n",
    "\n",
    "3. **Fit the Linear Regression Model**:\n",
    "   - `lr = LinearRegression()`:\n",
    "     - An instance of the `LinearRegression` class from Scikit-learn is created.\n",
    "   - `lr.fit(x, y)`:\n",
    "     - The model is trained (fitted) on the data. It calculates the optimal slope (`coef_`) and intercept (`intercept_`) that minimize the residual sum of squares between the observed `y` values and the predicted values.\n",
    "\n",
    "4. **Print Model Parameters**:\n",
    "   - `print(lr.intercept_, lr.coef_)`:\n",
    "     - The intercept and slope of the fitted regression line are printed. These parameters define the equation of the line: `y = intercept + slope * x`.\n",
    "\n",
    "5. **Visualize the Data and Regression Line**:\n",
    "   - `plt.scatter(x, y)`:\n",
    "     - A scatter plot of the data points is created, showing the relationship between `x` and `y`.\n",
    "   - `abline(lr.intercept_, lr.coef_, 'tab:orange')`:\n",
    "     - The `abline` function is called to plot the regression line using the calculated intercept and slope. The line is displayed in orange with a dashed style.\n",
    "   - `plt.ylim((0, 40))`:\n",
    "     - The y-axis limits are set to range from 0 to 40 to ensure the plot is appropriately scaled.\n",
    "   - `plt.show()`:\n",
    "     - The plot is displayed.\n",
    "\n",
    "### Purpose:\n",
    "This code illustrates how to perform simple linear regression on a dataset and visualize the results. The scatter plot shows the data points, while the regression line represents the best-fit line that models the relationship between `x` and `y`. The added noise in `y` simulates real-world data variability, making this a practical example of regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.1629596] [[2.0530771]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x    = np.random.uniform(0, 10, 20)\n",
    "y    = 10 + x * 2 + np.random.randn(20) * 2\n",
    "x, y = x.reshape(-1, 1), y.reshape(-1, 1)\n",
    "lr   = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "print(lr.intercept_, lr.coef_)\n",
    "plt.scatter(x, y)\n",
    "abline(lr.intercept_, lr.coef_, 'tab:orange')\n",
    "#abline(25, -1, 'tab:green')\n",
    "plt.ylim((0, 40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "Each data point now has many values (dimensions), not just X.\n",
    "\n",
    "We still want to predict a single Y by finding an optimal vector of coefficients.\n",
    "\n",
    "$$\n",
    "y = b + w_1x_1 + w_2x_2 + \\ldots + w_nx_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = w_01 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n\n",
    "$$\n",
    "This code snippet demonstrates the process of performing multiple linear regression using synthetic data. It generates a dataset with three independent variables (features) and one dependent variable (target), fits a linear regression model to the data, and outputs the model's parameters.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Generate Synthetic Data**:\n",
    "   - `n = 100`:\n",
    "     - Specifies the number of data points (samples) to generate.\n",
    "   - `X = np.random.uniform(0, 1, 3 * n).reshape((n, -1))`:\n",
    "     - Creates a 2D array `X` with `n` rows and 3 columns, where each element is a random value uniformly distributed between 0 and 1. This represents the feature matrix with three independent variables.\n",
    "   - `y = 10 + X[:, 0] * 2 + X[:, 1] * 5 + X[:, 2] * (-3) + np.random.randn(n) * 1`:\n",
    "     - Generates the target variable `y` using a linear equation:\n",
    "       - The intercept is `10`.\n",
    "       - The coefficients for the three features are `2`, `5`, and `-3`, respectively.\n",
    "       - Random noise is added to `y` using `np.random.randn(n) * 1`, which introduces variability to simulate real-world data.\n",
    "\n",
    "2. **Fit the Linear Regression Model**:\n",
    "   - `lr = LinearRegression()`:\n",
    "     - Creates an instance of the `LinearRegression` class from Scikit-learn. This class implements ordinary least squares regression.\n",
    "   - `lr.fit(X, y)`:\n",
    "     - Fits the linear regression model to the data. The `fit` method calculates the optimal coefficients (`coef_`) and intercept (`intercept_`) that minimize the residual sum of squares between the observed `y` values and the predicted values.\n",
    "\n",
    "3. **Output the Model Parameters**:\n",
    "   - `print(lr.intercept_, lr.coef_)`:\n",
    "     - Prints the intercept and coefficients of the fitted model. These parameters define the equation of the regression line:\n",
    "       ```\n",
    "       y = intercept + coef_[0] * X[:, 0] + coef_[1] * X[:, 1] + coef_[2] * X[:, 2]\n",
    "       ```\n",
    "     - The intercept and coefficients should closely match the values used to generate `y` (i.e., `10`, `2`, `5`, and `-3`), with slight deviations due to the added noise.\n",
    "\n",
    "### Purpose:\n",
    "This code illustrates how to perform multiple linear regression on a dataset with multiple features. By generating synthetic data with known parameters, it provides a controlled environment to verify the accuracy of the regression model. The fitted model can then be used to make predictions or analyze the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept   :  9.479223262258262\n",
      "Coefficients:  [ 2.37551194  5.3573248  -2.58798737]\n"
     ]
    }
   ],
   "source": [
    "n  = 100\n",
    "X  = np.random.uniform(0, 1, 3 * n).reshape((n, -1))\n",
    "y  = 10 + X[:, 0] * 2 + X[:, 1] * 5 + X[:, 2] * (-3) + np.random.randn(n) * 1\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print('Intercept   : ', lr.intercept_)\n",
    "print('Coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet creates a Pandas DataFrame to organize the synthetic data generated earlier for multiple linear regression. It structures the independent variables (`x1`, `x2`, `x3`) and the dependent variable (`y`) into a tabular format, making it easier to analyze and manipulate the data.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Create a DataFrame for Independent Variables**:\n",
    "   - `pd.DataFrame(X, columns=['x1', 'x2', 'x3'])`:\n",
    "     - The `X` array, which contains the three independent variables, is converted into a Pandas DataFrame.\n",
    "     - The `columns` parameter assigns meaningful names (`x1`, `x2`, `x3`) to the columns, corresponding to the three features in the dataset. This makes the data more interpretable and easier to work with.\n",
    "\n",
    "2. **Add the Dependent Variable**:\n",
    "   - `df['y'] = pd.Series(y)`:\n",
    "     - The target variable `y` is added as a new column in the DataFrame. The `pd.Series(y)` ensures that `y` is properly formatted as a Pandas Series before being added to the DataFrame.\n",
    "     - This step associates the dependent variable (`y`) with its corresponding independent variables (`x1`, `x2`, `x3`) in the same row.\n",
    "\n",
    "3. **Display the DataFrame**:\n",
    "   - `df`:\n",
    "     - Simply referencing the DataFrame in a Jupyter Notebook cell outputs its contents, allowing you to inspect the data. This is useful for verifying that the data has been correctly structured.\n",
    "\n",
    "### Purpose:\n",
    "This code organizes the synthetic dataset into a structured format that is ideal for further analysis, such as fitting regression models or performing exploratory data analysis. By using a DataFrame, you can easily access, manipulate, and visualize the data, which is essential for tasks like statistical modeling or machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df      = pd.DataFrame(X, columns=['x1', 'x2', 'x3'])\n",
    "df['y'] = pd.Series(y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to perform multiple linear regression using the `statsmodels` library and obtain a detailed summary of the model's results.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Define the Regression Model**:\n",
    "   - `smf.ols('y ~ x1 + x2 + x3', data=df)`:\n",
    "     - The `ols` function from `statsmodels.formula.api` is used to define an Ordinary Least Squares (OLS) regression model.\n",
    "     - The formula `'y ~ x1 + x2 + x3'` specifies the relationship between the dependent variable (`y`) and the independent variables (`x1`, `x2`, and `x3`). The tilde (`~`) separates the dependent variable from the independent variables, and the `+` operator indicates that all three independent variables are included in the model.\n",
    "     - The `data=df` argument specifies that the variables `y`, `x1`, `x2`, and `x3` are columns in the Pandas DataFrame `df`.\n",
    "\n",
    "2. **Fit the Model**:\n",
    "   - `.fit()`:\n",
    "     - This method fits the OLS regression model to the data by estimating the coefficients (intercept and slopes) that minimize the residual sum of squares. Internally, it uses numerical techniques such as the Moore-Penrose pseudoinverse or QR decomposition to solve the least squares problem.\n",
    "\n",
    "3. **Generate the Model Summary**:\n",
    "   - `model.summary()`:\n",
    "     - This method generates a comprehensive summary of the fitted regression model. The summary includes:\n",
    "       - **Coefficients**: The estimated values for the intercept and slopes of the independent variables.\n",
    "       - **P-values**: Statistical significance of each coefficient, indicating whether the variable has a meaningful relationship with the dependent variable.\n",
    "       - **R-squared and Adjusted R-squared**: Measures of how well the independent variables explain the variability in the dependent variable.\n",
    "       - **F-statistic**: A test statistic for the overall significance of the model.\n",
    "       - **Standard Errors**: The variability of the coefficient estimates.\n",
    "       - **Residual Statistics**: Information about the distribution of the residuals (errors).\n",
    "\n",
    "### Purpose:\n",
    "This code is used to build and evaluate a multiple linear regression model. The `model.summary()` output provides insights into the relationships between the dependent and independent variables, the model's overall performance, and the statistical significance of each predictor. This information is crucial for interpreting the model and making data-driven decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y ~ x1 + x2 + x3', data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R<sup>2</sup>: coefficient of determination\n",
    "\n",
    "$R^2 = 1 - \\frac{\\textrm{unexplained variability}}{\\textrm{total variability}} = 1 - \\frac{SSR}{SST}$\n",
    "\n",
    "$R^2_{adj} = 1 - \\frac{SSR}{SST} \\cdot \\frac{n - 1}{n - k - 1} $\n",
    "\n",
    "This code snippet calculates and prints both the **RÂ² (coefficient of determination)** and the **adjusted RÂ²** for a regression model. These metrics are essential for evaluating the performance of a regression model, particularly in explaining the variability of the dependent variable.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Retrieve RÂ² Value**:\n",
    "   - `R2 = model.rsquared`:\n",
    "     - The `rsquared` attribute of the `model` object (from `statsmodels`) retrieves the RÂ² value of the fitted regression model. RÂ² measures the proportion of the variance in the dependent variable (`y`) that is explained by the independent variables (`x1`, `x2`, `x3`). It ranges from 0 to 1, where higher values indicate better model performance.\n",
    "\n",
    "2. **Define Adjusted RÂ² Calculation**:\n",
    "   - `def R2_adjust(r2, n, k):`:\n",
    "     - This function calculates the **adjusted RÂ²**, which adjusts the RÂ² value to account for the number of predictors (`k`) and the sample size (`n`). Adjusted RÂ² penalizes the inclusion of irrelevant predictors, making it a more reliable metric for comparing models with different numbers of predictors.\n",
    "   - The formula used is:\n",
    "     ```\n",
    "     Adjusted RÂ² = 1 - ((1 - RÂ²) * (n - 1) / (n - k - 1))\n",
    "     ```\n",
    "     - `n`: Total number of observations (sample size).\n",
    "     - `k`: Number of predictors (independent variables).\n",
    "     - The adjustment ensures that adding more predictors to the model does not artificially inflate the RÂ² value.\n",
    "\n",
    "3. **Calculate Adjusted RÂ²**:\n",
    "   - `adjR2 = R2_adjust(R2, 100, 3)`:\n",
    "     - The `R2_adjust` function is called with:\n",
    "       - `R2`: The RÂ² value of the model.\n",
    "       - `n = 100`: The total number of observations in the dataset.\n",
    "       - `k = 3`: The number of independent variables in the model (`x1`, `x2`, `x3`).\n",
    "     - The function returns the adjusted RÂ² value, which accounts for the model's complexity.\n",
    "\n",
    "4. **Print RÂ² and Adjusted RÂ²**:\n",
    "   - `print('R2         : ', R2)`:\n",
    "     - Outputs the RÂ² value, showing how much of the variability in the dependent variable is explained by the model.\n",
    "   - `print('Adjusted R2: ', adjR2)`:\n",
    "     - Outputs the adjusted RÂ² value, which provides a more nuanced evaluation by penalizing unnecessary predictors.\n",
    "\n",
    "### Purpose:\n",
    "The RÂ² value indicates how well the model explains the variability in the dependent variable, while the adjusted RÂ² provides a more accurate assessment by considering the number of predictors and the sample size. Together, these metrics help in assessing the quality and efficiency of the regression model, particularly when comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = model.rsquared\n",
    "def R2_adjust(r2, n, k):\n",
    "    \"\"\"Calculate adjusted R2\"\"\"\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "adjR2 = R2_adjust(R2, 100, 3)\n",
    "print('R2         : ',R2)\n",
    "print('Adjusted R2: ',adjR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet prints the **adjusted RÂ² (RÂ² adjusted)** value of a regression model. The adjusted RÂ² is a statistical metric that evaluates the goodness of fit of a regression model while accounting for the number of predictors used.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Accessing Adjusted RÂ²**:\n",
    "   - `model.rsquared_adj`:\n",
    "     - This attribute of the `model` object (from `statsmodels`) retrieves the adjusted RÂ² value of the fitted regression model. Unlike the standard RÂ², which measures the proportion of variance in the dependent variable explained by the independent variables, the adjusted RÂ² adjusts for the number of predictors in the model. It penalizes the inclusion of irrelevant predictors that do not improve the model's explanatory power.\n",
    "\n",
    "2. **Printing the Adjusted RÂ²**:\n",
    "   - `print('RS Square Adj: ', model.rsquared_adj)`:\n",
    "     - The `print` function outputs a descriptive label (`'RS Square Adj: '`) followed by the value of `model.rsquared_adj`. This makes it easy to interpret the adjusted RÂ² value in the context of the regression analysis.\n",
    "\n",
    "### Purpose:\n",
    "The adjusted RÂ² is particularly useful when comparing regression models with different numbers of predictors. It provides a more reliable measure of model performance by accounting for model complexity. A higher adjusted RÂ² indicates a better balance between explanatory power and simplicity, making it a key metric for evaluating and selecting regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RS Square Adj: ', model.rsquared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error\n",
    "\n",
    "$MSE = \\frac{SSR}{n}$\n",
    "\n",
    "$RMSE = \\sqrt{MSE}$\n",
    "\n",
    "This code snippet demonstrates how to evaluate the performance of a simple linear regression model by calculating two common error metrics: **Root Mean Squared Error (RMSE)** and **Mean Absolute Error (MAE)**. These metrics quantify the difference between the predicted and actual values, providing insight into the model's accuracy.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Generate Synthetic Data**:\n",
    "   - `x = np.random.uniform(0, 10, 20)`:\n",
    "     - Generates 20 random values for `x` uniformly distributed between 0 and 10.\n",
    "   - `y = 10 + x * 2 + np.random.randn(20) * 2`:\n",
    "     - Creates the corresponding `y` values using the linear equation `y = 10 + 2x` with added random noise. The noise is drawn from a standard normal distribution (`np.random.randn`) and scaled by a factor of 2 to simulate variability in real-world data.\n",
    "\n",
    "2. **Reshape Data**:\n",
    "   - `x, y = x.reshape(-1, 1), y.reshape(-1, 1)`:\n",
    "     - Reshapes the 1D arrays `x` and `y` into 2D column vectors. This is necessary because the `LinearRegression` model expects 2D input for both features (`x`) and target values (`y`).\n",
    "\n",
    "3. **Fit the Linear Regression Model**:\n",
    "   - `lr = LinearRegression()`:\n",
    "     - Creates an instance of the `LinearRegression` class from Scikit-learn.\n",
    "   - `lr.fit(x, y)`:\n",
    "     - Fits the linear regression model to the data. The model learns the optimal slope (`coef_`) and intercept (`intercept_`) that minimize the residual sum of squares between the observed `y` values and the predicted values.\n",
    "\n",
    "4. **Calculate RMSE**:\n",
    "   - `rmse = np.sqrt(np.mean((lr.predict(x) - y)**2))`:\n",
    "     - Computes the **Root Mean Squared Error (RMSE)**:\n",
    "       - `lr.predict(x)` generates predictions for the input `x`.\n",
    "       - `(lr.predict(x) - y)` calculates the residuals (differences between predicted and actual values).\n",
    "       - Squaring the residuals, taking their mean, and then applying the square root yields the RMSE.\n",
    "     - RMSE penalizes larger errors more heavily, making it sensitive to outliers.\n",
    "\n",
    "5. **Calculate MAE**:\n",
    "   - `mae = np.mean(abs(lr.predict(x) - y))`:\n",
    "     - Computes the **Mean Absolute Error (MAE)**:\n",
    "       - `abs(lr.predict(x) - y)` calculates the absolute residuals.\n",
    "       - Taking the mean of these absolute residuals gives the MAE.\n",
    "     - MAE provides a straightforward measure of the average error magnitude, treating all errors equally.\n",
    "\n",
    "6. **Print the Metrics**:\n",
    "   - `print('RMSE: ', rmse)`:\n",
    "     - Outputs the RMSE value, which indicates the model's overall error magnitude with a higher sensitivity to large errors.\n",
    "   - `print('MAE: ', mae)`:\n",
    "     - Outputs the MAE value, which provides a simpler measure of the average error magnitude.\n",
    "\n",
    "### Purpose:\n",
    "This code evaluates the accuracy of the linear regression model using RMSE and MAE. RMSE is useful for understanding the impact of large errors, while MAE provides a more intuitive measure of average error. Together, these metrics help assess the model's performance and its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x    = np.random.uniform(0, 10, 20)\n",
    "y    = 10 + x * 2 + np.random.randn(20) * 2\n",
    "x, y = x.reshape(-1, 1), y.reshape(-1, 1)\n",
    "lr   = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "rmse = np.sqrt(np.mean((lr.predict(x) - y)**2))\n",
    "mae  = np.mean(abs(lr.predict(x) - y))\n",
    "print('RMSE: ',rmse)\n",
    "print('MAE : ',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the coefficients\n",
    "### Closed form solution\n",
    "\n",
    "$$\\vec{w}= (X^TX)^{-1}X^T\\vec{y}$$\n",
    "### Gradient descent\n",
    "This code snippet demonstrates the process of generating synthetic data, making predictions using a simple linear model, and calculating the **Mean Squared Error (MSE)** to evaluate the model's performance. It highlights the initialization of a model with a coefficient of zero and the computation of the error between predictions and actual values.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Generate Synthetic Data**:\n",
    "   - `n = 20`:\n",
    "     - Specifies the number of data points to generate.\n",
    "   - `x = np.random.uniform(0, 10, n)`:\n",
    "     - Generates 20 random values for `x` uniformly distributed between 0 and 10. This serves as the independent variable.\n",
    "   - `y = 0 + x * 2 + np.random.randn(n) * 2`:\n",
    "     - Creates the dependent variable `y` using the linear equation `y = 0 + 2x` with added random noise. The noise is drawn from a standard normal distribution (`np.random.randn`) and scaled by a factor of 2 to simulate variability in real-world data.\n",
    "\n",
    "2. **Reshape Data**:\n",
    "   - `x, y = x.reshape(-1, 1), y.reshape(-1, 1)`:\n",
    "     - Reshapes the 1D arrays `x` and `y` into 2D column vectors. This is necessary for compatibility with the subsequent operations, as many machine learning models expect 2D input.\n",
    "\n",
    "3. **Initialize the Model**:\n",
    "   - `coef = 0`:\n",
    "     - Initializes the coefficient of the linear model to zero. This means the model initially predicts `y` as zero for all values of `x`.\n",
    "\n",
    "4. **Define the Prediction Function**:\n",
    "   - `def predict(x, coef): return x * coef`:\n",
    "     - This function computes predictions for `y` based on the input `x` and the model's coefficient `coef`. Since `coef` is initialized to zero, all predictions will initially be zero.\n",
    "\n",
    "5. **Define the MSE Function**:\n",
    "   - `def mse(y_pred, y): return np.mean((y_pred - y)**2) / 2`:\n",
    "     - This function calculates the **Mean Squared Error (MSE)**, a common metric for evaluating regression models. It computes the average of the squared differences between the predicted values (`y_pred`) and the actual values (`y`), divided by 2 for mathematical convenience.\n",
    "\n",
    "6. **Make Predictions and Calculate MSE**:\n",
    "   - `y_pred = predict(x, coef)`:\n",
    "     - Generates predictions for `y` using the initialized coefficient (`coef = 0`).\n",
    "   - `mse(y_pred, y)`:\n",
    "     - Computes the MSE between the predictions (`y_pred`) and the actual values (`y`). Since the predictions are all zero, the MSE will reflect the average squared deviation of the actual values from zero.\n",
    "\n",
    "7. **Print Results**:\n",
    "   - `print('Predictions: ', y_pred)`:\n",
    "     - Outputs the predicted values, which are all zeros due to the initialized coefficient.\n",
    "   - `print('MSE        : ', mse(y_pred, y))`:\n",
    "     - Outputs the calculated MSE, providing a measure of how far the predictions are from the actual values.\n",
    "\n",
    "### Purpose:\n",
    "This code serves as a foundational example for understanding how to generate synthetic data, make predictions using a simple linear model, and evaluate the model's performance using MSE. It demonstrates the importance of initializing model parameters and highlights the role of error metrics in assessing model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n    = 20\n",
    "x    = np.random.uniform(0, 10, n)\n",
    "y    = 0 + x * 2 + np.random.randn(n) * 2\n",
    "x, y = x.reshape(-1, 1), y.reshape(-1, 1)\n",
    "coef = 0\n",
    "def predict(x, coef):\n",
    "    return x * coef\n",
    "def mse(y_pred, y):\n",
    "    return np.mean((y_pred - y)**2) / 2\n",
    "y_pred = predict(x, coef)\n",
    "print('Predictions: ', y_pred)\n",
    "print('MSE        : ',mse(y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet visualizes the relationship between the coefficient of a linear model and its corresponding **Mean Squared Error (MSE)**. It generates a plot that helps evaluate how the choice of the coefficient affects the model's performance.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Generate Coefficient Values**:\n",
    "   - `coefs = np.linspace(-2, 6, 100)`:\n",
    "     - This creates an array of 100 evenly spaced values between -2 and 6. These values represent potential coefficients for the linear model. The range is chosen to explore how different coefficients impact the model's error.\n",
    "\n",
    "2. **Initialize the Cost List**:\n",
    "   - `cost = []`:\n",
    "     - An empty list is created to store the MSE values corresponding to each coefficient.\n",
    "\n",
    "3. **Iterate Over Coefficients**:\n",
    "   - `for c in coefs:`:\n",
    "     - A loop iterates through each coefficient value in the `coefs` array.\n",
    "   - `y_pred = predict(x, c)`:\n",
    "     - For each coefficient `c`, the `predict` function is called to compute the predicted values (`y_pred`) based on the input data `x` and the current coefficient.\n",
    "   - `cost.append(mse(y_pred, y))`:\n",
    "     - The **Mean Squared Error (MSE)** between the predicted values (`y_pred`) and the actual values (`y`) is calculated using the `mse` function. The resulting error is appended to the `cost` list.\n",
    "\n",
    "4. **Plot the Results**:\n",
    "   - `plt.plot(coefs, cost)`:\n",
    "     - A line plot is created with the coefficient values (`coefs`) on the x-axis and the corresponding MSE values (`cost`) on the y-axis. This visualizes how the error changes as the coefficient varies.\n",
    "   - `plt.xlabel('Coefficient')`:\n",
    "     - Labels the x-axis as \"Coefficient\" to indicate the variable being adjusted.\n",
    "   - `plt.ylabel('Cost (MSE)')`:\n",
    "     - Labels the y-axis as \"Cost (MSE)\" to represent the error metric being evaluated.\n",
    "   - `plt.show()`:\n",
    "     - Displays the plot.\n",
    "\n",
    "### Purpose:\n",
    "This code helps identify the coefficient value that minimizes the MSE, which corresponds to the best fit for the linear model. The plot provides a visual representation of the error landscape, making it easier to understand the relationship between the coefficient and the model's performance. This is particularly useful for tasks like parameter tuning or understanding the behavior of a cost function in optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.linspace(-2, 6, 100)\n",
    "cost  = []\n",
    "for c in coefs:\n",
    "    y_pred = predict(x, c)\n",
    "    cost.append(mse(y_pred, y))\n",
    "plt.plot(coefs, cost)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet implements a basic gradient descent algorithm to optimize the coefficient of a simple linear regression model. The goal is to iteratively adjust the coefficient (`coef`) to minimize the error between the predicted and actual values.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Gradient Function**:\n",
    "   - `def gradient(x, y, y_pred): return np.mean((y_pred - y) * x)`:\n",
    "     - This function calculates the gradient of the cost function with respect to the coefficient. The gradient represents the direction and magnitude of change needed to reduce the error.\n",
    "     - `(y_pred - y)` computes the residuals (differences between predicted and actual values).\n",
    "     - Multiplying the residuals by `x` scales the gradient based on the input values.\n",
    "     - `np.mean` averages the gradients across all data points, providing a single value to update the coefficient.\n",
    "\n",
    "2. **Update Function**:\n",
    "   - `def update(grad, rate): return rate * grad`:\n",
    "     - This function scales the gradient by the learning rate (`rate`), which controls the step size of the updates.\n",
    "     - A smaller learning rate results in smaller updates, leading to slower but more stable convergence. A larger rate speeds up convergence but risks overshooting the optimal value.\n",
    "\n",
    "3. **Fit Function**:\n",
    "   - `def fit(x, y, coef=0, n_iter=100, rate=0.01):`:\n",
    "     - This function performs the gradient descent optimization.\n",
    "     - `coef=0`: Initializes the coefficient to zero.\n",
    "     - `n_iter=100`: Specifies the number of iterations for the optimization process.\n",
    "     - `rate=0.01`: Sets the learning rate for updating the coefficient.\n",
    "\n",
    "4. **Gradient Descent Loop**:\n",
    "   - `for i in range(n_iter):`:\n",
    "     - Iterates through the specified number of iterations.\n",
    "   - `y_pred = predict(x, coef)`:\n",
    "     - Computes the predicted values using the current coefficient.\n",
    "   - `grad = gradient(x, y, y_pred)`:\n",
    "     - Calculates the gradient of the cost function using the current predictions.\n",
    "   - `coef -= update(grad, rate)`:\n",
    "     - Updates the coefficient by subtracting the scaled gradient. This step moves the coefficient closer to the value that minimizes the error.\n",
    "\n",
    "5. **Return the Optimized Coefficient**:\n",
    "   - `return coef`:\n",
    "     - After completing all iterations, the function returns the optimized coefficient.\n",
    "\n",
    "6. **Function Call**:\n",
    "   - `fit(x, y)`:\n",
    "     - Executes the gradient descent process on the input data `x` and `y`.\n",
    "\n",
    "### Purpose:\n",
    "This code demonstrates how gradient descent can be used to optimize a simple linear regression model. By iteratively adjusting the coefficient based on the gradient, the algorithm minimizes the error between the predicted and actual values. This approach is foundational in machine learning and serves as a building block for more complex optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, y_pred):\n",
    "    return np.mean((y_pred - y) * x)\n",
    "def update(grad, rate):\n",
    "    return rate * grad\n",
    "def fit(x, y, coef=0, n_iter=100, rate=0.01):\n",
    "    for i in range(n_iter):\n",
    "        #print(coef)\n",
    "        y_pred = predict(x, coef)\n",
    "        #print(mse(y_pred, y))\n",
    "        grad = gradient(x, y, y_pred)\n",
    "        #print(grad)\n",
    "        coef -= update(grad, rate)\n",
    "    return coef\n",
    "print('Coefficient: ', fit(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet generates synthetic data for a simple linear regression problem and prepares the data for modeling by adding a bias term (intercept) to the feature matrix. Here's a step-by-step explanation:\n",
    "\n",
    "1. **Generate Synthetic Data**:\n",
    "   - `n = 20`:\n",
    "     - Specifies the number of data points to generate.\n",
    "   - `x = np.random.uniform(0, 10, n)`:\n",
    "     - Creates an array of 20 random values for the independent variable `x`, uniformly distributed between 0 and 10. This simulates a range of input values for the regression model.\n",
    "   - `y = -5 + x * 2 + np.random.randn(n) * 2`:\n",
    "     - Generates the dependent variable `y` using the linear equation `y = -5 + 2x` with added noise. The noise is drawn from a standard normal distribution (`np.random.randn`) and scaled by a factor of 2 to introduce variability, mimicking real-world data.\n",
    "\n",
    "2. **Reshape Data**:\n",
    "   - `x, y = x.reshape(-1, 1), y.reshape(-1, 1)`:\n",
    "     - Reshapes the 1D arrays `x` and `y` into 2D column vectors with one column and multiple rows. This format is required for compatibility with many machine learning algorithms, which expect 2D input.\n",
    "\n",
    "3. **Add Bias Term**:\n",
    "   - `x = np.concatenate([np.ones(n).reshape(-1, 1), x], axis=1)`:\n",
    "     - Adds a column of ones to the `x` array. This column represents the bias term (intercept) in the regression model. By including this column, the model can learn the intercept (`-5` in this case) as part of the optimization process.\n",
    "     - The `np.ones(n).reshape(-1, 1)` creates a column vector of ones with `n` rows, and `np.concatenate` combines it with the original `x` along the second axis (`axis=1`), resulting in a 2D array where the first column is all ones and the second column contains the original `x` values.\n",
    "\n",
    "4. **Print the Result**:\n",
    "   - `print('x: ', x)`:\n",
    "     - Outputs the modified `x` array to the console. The printed array will show two columns: the first column of ones (bias term) and the second column of the original `x` values.\n",
    "\n",
    "### Purpose:\n",
    "This code prepares the data for a linear regression model by generating synthetic input-output pairs and augmenting the feature matrix with a bias term. The inclusion of the bias term ensures that the model can learn both the slope and the intercept of the regression line. This setup is commonly used in machine learning and statistical modeling to handle linear relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n    = 20\n",
    "x    = np.random.uniform(0, 10, n)\n",
    "y    = -5 + x * 2 + np.random.randn(n) * 2\n",
    "x, y = x.reshape(-1, 1), y.reshape(-1, 1)\n",
    "x    = np.concatenate([np.ones(n).reshape(-1, 1), x], axis=1)\n",
    "print('x:\\n',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the process of optimizing the coefficients of a linear regression model using gradient descent. It includes functions for making predictions, calculating gradients, and fitting the model to the data.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Prediction Function**:\n",
    "   - `def predict(x, coef):`:\n",
    "     - This function computes the predicted values (`y_pred`) for the input data `x` using the given coefficients `coef`.\n",
    "   - `np.dot(x, coef)`:\n",
    "     - The dot product is used to calculate the linear combination of the input features (`x`) and the coefficients (`coef`). This operation corresponds to the equation of a linear model:  \n",
    "       ```\n",
    "       y_pred = coef[0] * 1 + coef[1] * x[1] + ... + coef[n] * x[n]\n",
    "       ```\n",
    "       Here, the first column of `x` is typically a column of ones to account for the intercept term.\n",
    "   - `.reshape(-1, 1)`:\n",
    "     - Ensures the output is reshaped into a 2D column vector, which is the expected format for further calculations.\n",
    "\n",
    "2. **Gradient Calculation**:\n",
    "   - `def gradient(x, y, y_pred):`:\n",
    "     - This function calculates the gradient of the cost function (Mean Squared Error) with respect to the coefficients.\n",
    "   - `(y_pred - y) * x`:\n",
    "     - Computes the element-wise product of the residuals (`y_pred - y`) and the input features (`x`). This represents the contribution of each feature to the gradient.\n",
    "   - `np.mean(..., axis=0)`:\n",
    "     - Averages the gradients across all data points to obtain the overall gradient for each coefficient.\n",
    "\n",
    "3. **Fitting the Model**:\n",
    "   - `coef = fit(x, y, coef=np.zeros(2), n_iter=10000):`:\n",
    "     - The `fit` function is called to optimize the coefficients using gradient descent.\n",
    "     - `coef=np.zeros(2)`:\n",
    "       - Initializes the coefficients to zero. Since the input `x` includes a bias term (intercept), there are two coefficients: one for the intercept and one for the slope.\n",
    "     - `n_iter=10000`:\n",
    "       - Specifies the number of iterations for the gradient descent algorithm. This determines how many times the coefficients are updated.\n",
    "\n",
    "4. **Printing the Results**:\n",
    "   - `print('coef :', coef)`:\n",
    "     - Outputs the optimized coefficients after the gradient descent process. These coefficients represent the best-fit line for the given data.\n",
    "\n",
    "### Purpose:\n",
    "This code implements the core components of a linear regression model trained using gradient descent. The `predict` function generates predictions based on the current coefficients, the `gradient` function calculates the direction and magnitude of updates needed to minimize the error, and the `fit` function iteratively adjusts the coefficients to find the optimal values. The final coefficients can be used to make predictions or interpret the relationship between the input features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, coef):\n",
    "    # Because 1*coef[0] + x[1]*coef[1] + ... +  x[n]*coef[n] is the dot product coef Â· x\n",
    "    return np.dot(x, coef).reshape(-1, 1)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.mean((y_pred - y) * x, axis=0)\n",
    "coef = fit(x, y, coef=np.zeros(2), n_iter=10000)\n",
    "print('coef :',coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the use of the `LinearRegression` class from the Scikit-learn library to fit a linear regression model to a dataset and display the resulting model parameters.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Model Initialization and Fitting**:\n",
    "   - `lr = LinearRegression().fit(x[:, 1:], y)`:\n",
    "     - An instance of the `LinearRegression` class is created. By default, this class performs Ordinary Least Squares (OLS) regression to find the best-fit line for the data.\n",
    "     - The `fit` method is called with the input features `x[:, 1:]` and the target variable `y`. \n",
    "       - `x[:, 1:]` selects all rows of `x` but excludes the first column. This is because the first column of `x` typically contains the bias term (a column of ones) added during preprocessing.\n",
    "       - The `fit` method computes the optimal coefficients (`coef_`) and intercept (`intercept_`) that minimize the residual sum of squares between the observed and predicted values.\n",
    "\n",
    "2. **Printing the Model Parameters**:\n",
    "   - `print('Intercept   : ', lr.intercept_)`:\n",
    "     - Outputs the intercept of the fitted regression model. The intercept represents the value of the target variable `y` when all input features are zero.\n",
    "   - `print('Coefficients: ', lr.coef_)`:\n",
    "     - Outputs the coefficients of the fitted model. Each coefficient corresponds to the weight assigned to a specific feature in the input data. These coefficients indicate the strength and direction of the relationship between each feature and the target variable.\n",
    "\n",
    "### Purpose:\n",
    "This code fits a linear regression model to the provided dataset and retrieves the model parameters (intercept and coefficients). These parameters define the equation of the regression line, which can be used to make predictions or interpret the relationships between the input features and the target variable. By excluding the bias term from the input features (`x[:, 1:]`), the model ensures that the intercept is handled separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(x[:, 1:], y)\n",
    "print('Intercept   : ', lr.intercept_)\n",
    "print('Coefficients: ', lr.coef_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet visualizes the results of a linear regression model by plotting the data points and the fitted regression line. It uses Matplotlib's `scatter`, `plot`, and `show` functions to create and display the visualization.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Scatter Plot of Data Points**:\n",
    "   - `plt.scatter(x[:, 1], y)`:\n",
    "     - This creates a scatter plot of the data points, where `x[:, 1]` represents the independent variable (feature) and `y` represents the dependent variable (target).\n",
    "     - The `x[:, 1]` selects the second column of the `x` array, which contains the actual feature values (excluding the bias term in the first column). This ensures that the scatter plot reflects the relationship between the feature and the target variable.\n",
    "\n",
    "2. **Plotting the Regression Line**:\n",
    "   - `plt.plot(x[:, 1], predict(x, coef), 'tab:orange')`:\n",
    "     - This plots the fitted regression line over the scatter plot. The `predict` function is used to calculate the predicted `y` values based on the input `x` and the optimized coefficients `coef`.\n",
    "     - The `x[:, 1]` values are used as the x-coordinates, while the predicted values from `predict(x, coef)` are used as the y-coordinates.\n",
    "     - The `'tab:orange'` argument specifies the color of the regression line, making it visually distinct from the scatter plot.\n",
    "\n",
    "3. **Displaying the Plot**:\n",
    "   - `plt.show()`:\n",
    "     - This displays the plot in a graphical window or inline (e.g., in a Jupyter Notebook). It ensures that the scatter plot and regression line are rendered for visualization.\n",
    "\n",
    "### Purpose:\n",
    "This visualization helps to evaluate the performance of the linear regression model by showing how well the fitted regression line aligns with the actual data points. The scatter plot represents the observed data, while the regression line represents the model's predictions. A good fit is indicated by the regression line closely following the trend of the data points. This type of plot is commonly used to assess the quality of a regression model and to interpret the relationship between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 1], y)\n",
    "plt.plot(x[:, 1], predict(x, coef), 'tab:orange')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
